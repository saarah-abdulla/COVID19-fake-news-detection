{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "combined-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cdc currently reports 99031 deaths general discrepancies death counts different sources small explicable death toll stands roughly 100000 people today', 1], ['states reported 1121 deaths small rise last tuesday southern states reported 640 deaths', 1], ['politically correct woman almost uses pandemic excuse reuse plastic bag coronavirus nashville', 0]]\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing and data loading functions\n",
    "# Sample dataset with first 4 rows from the corresponding excel files\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Loading dataset from excel\n",
    "\n",
    "Return:\n",
    "    Data file read from Pandas    \n",
    "\"\"\"\n",
    "\n",
    "train = pd.read_excel('../dataset/Project_Datasets/Constraint_English_Train.xlsx')\n",
    "train = train[0:3]\n",
    "val = pd.read_excel('../dataset/Project_Datasets/Constraint_English_Val.xlsx')\n",
    "val = val[0:3]\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Clean Text function\n",
    "\n",
    "Args:\n",
    "    string: Each line of tweets from the excel file\n",
    "    \n",
    "Output:\n",
    "    The processed tweets according to cleanText criteria\n",
    "\"\"\"\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def cleanText(string):\n",
    "    text = string.lower().split() # change all the sentences to lower case\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text) # removes link    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text) # removes link\n",
    "    text = text.replace('&amp',' ') # remove &amp and replace with space\n",
    "    text = re.sub(r\"&\",' and ',text) # replace the symbol \"&\" with the word \"and\"\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text) # removes all except regex (including emojis)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data loader function\n",
    "\n",
    "Args:\n",
    "    input_data: Dataset excel file read from Pandas\n",
    "    \n",
    "Output: \n",
    "    List of lists, each list is of [tweets, label]\n",
    "\"\"\"  \n",
    "\n",
    "def load_data(input_data):\n",
    "    output = []\n",
    "    for index, row in input_data.iterrows():\n",
    "        tweets = cleanText(row[1])\n",
    "        label = row[2]\n",
    "        if label == \"real\":\n",
    "            label_out = 1\n",
    "        else:\n",
    "            label_out = 0\n",
    "        output.append([tweets, label_out])\n",
    "    return output\n",
    "\n",
    "\n",
    "print(load_data(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "motivated-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Train, Val data in Pandas.DataFrame format\n",
    "\n",
    "train_data = load_data(train)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"tweets\", \"labels\"]\n",
    "\n",
    "val_data = load_data(val)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df.columns = [\"tweets\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abandoned-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8403ec473d4a1b99859c5634c3e3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b993332da140bcb7bf5fc689224814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06c9dc6ffd846a7a43840e0a5a8cf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86603fa28f434edc86675adc0dab4775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluate predication ------\n",
      " Test text - Ground truth - Prediction \n",
      "alfalfa cure covid 19 - fake - 0\n",
      "\n",
      "president trump asked would catch coronavirus donaldtrump coronavirus - fake - 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "\"\"\"\n",
    "The following set of hyperparameters are the optimized hyperparameters\n",
    "\"\"\"\n",
    "model_args = ClassificationArgs()\n",
    "model_args.train_batch_size = 64\n",
    "model_args.learning_rate = 0.0000215\n",
    "model_args.max_seq_length = 256\n",
    "model_args.num_train_epochs = 42\n",
    "model_args.overwrite_output_dir = True\n",
    "\n",
    "model = ClassificationModel(\n",
    "        'albert', \n",
    "        'albert-base-v2', \n",
    "        num_labels=2, \n",
    "        use_cuda=True, \n",
    "        args=model_args\n",
    "    )\n",
    "\n",
    "# Evaluating the Model\n",
    "\n",
    "# Loading the test dataset\n",
    "test = pd.read_excel('../dataset/Project_Datasets/english_test_with_labels.xlsx')\n",
    "test = test[1:3]\n",
    "\n",
    "\"\"\"\n",
    "Predict function \n",
    "\n",
    "Input:\n",
    "    input_data: The test dataset to be used for evaluation\n",
    "    \n",
    "Output:\n",
    "    List of lists with each line in the form of [tweet, ground_truth, prediction]\n",
    "        for line in the test dataset excel\n",
    "\"\"\"\n",
    "def predict(input_data):\n",
    "    output = []\n",
    "    for index, row in input_data.iterrows():        \n",
    "        tweets = cleanText(row[1])\n",
    "        gt_label= row[2]\n",
    "        predictions, raw_outputs = model.predict([tweets])\n",
    "        output.append([tweets, gt_label , predictions])\n",
    "    return output\n",
    "\n",
    "pred_output = predict(test)\n",
    "\n",
    "# For displaying the sample prediciton results \n",
    "print(\"------ Evaluate predication ------\")\n",
    "print(\" Test text - Ground truth - Prediction \")\n",
    "for item in pred_output:\n",
    "    print(\"%s - %s - %d\\n\" %(item[0], item[1], item[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-membrane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
